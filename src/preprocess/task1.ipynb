{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download DAIC-WOZ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from lxml import etree\n",
    "import time \n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.packages.urllib3.disable_warnings() # to disable the warning\n",
    "\n",
    "\n",
    "def create_soup(url):\n",
    "\n",
    "    user_agent = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "                }\n",
    "    resp = requests.get(url, headers=user_agent, verify=False)\n",
    "    if resp.ok:\n",
    "        return BeautifulSoup(resp.text,'html.parser')\n",
    "    else:\n",
    "        print('Error:',resp.status_code)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the download links for the DAIC-WOZ dataset\n",
    "DAIC_WOZ_page_url = r'https://dcapswoz.ict.usc.edu/wwwdaicwoz/'\n",
    "soup = create_soup(DAIC_WOZ_page_url)\n",
    "raw_data_url = soup.find_all('a',href=re.compile(r'_P.zip'))\n",
    "ids = [id.get('href') for id in raw_data_url]\n",
    "# https://dcapswoz.ict.usc.edu/wwwdaicwoz/311_P.zip\n",
    "urls = [DAIC_WOZ_page_url + id for id in ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the DAIC-WOZ dataset\n",
    "for url in urls:\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(f'.../data/raw/DAIC_WOZ/{url.split('/')[-1]}', 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    time.sleep(random.randint(1, 3)) # sleep for a while to avoid being blocked by the server\n",
    "    print(f'{url.split(\"/\")[-1]} downloaded')\n",
    "print('All files downloaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install with aiohttp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nest_asyncio\n",
    "import os\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# 允许嵌套事件循环\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def download_file(session, url, file_path, semaphore):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            async with session.get(url, timeout=aiohttp.ClientTimeout(total=60*60*60)) as response:  # 增加超时时间\n",
    "                os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    while True:\n",
    "                        chunk = await response.content.read(8192)\n",
    "                        if not chunk:\n",
    "                            break\n",
    "                        f.write(chunk)\n",
    "        except asyncio.TimeoutError:\n",
    "            print(f\"Timeout error for URL: {url}\")\n",
    "\n",
    "async def main(urls):\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)  # 设置最大并发数\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for url in urls:\n",
    "            file_path = f'../../data/raw/DAIC_WOZ/{url.split(\"/\")[-1]}'\n",
    "            tasks.append(download_file(session, url, file_path, semaphore))\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "max_concurrency = 5  # 最大并发数\n",
    "\n",
    "# 在 Jupyter Notebook 中运行异步函数\n",
    "await main(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip the dataset and keep necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the downloaded zip files and only keep the .wav and TRANSCRIPT.csv files\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the directory where the zip files are stored\n",
    "zip_dir = '../../data/raw/DAIC_WOZ/'\n",
    "\n",
    "# Define the directory where the unzipped files will be stored\n",
    "unzip_dir = '../../data/raw/DAIC_WOZ_unzipped/'\n",
    "\n",
    "# Delete the directory if it already exists\n",
    "shutil.rmtree(unzip_dir, ignore_errors=True)\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(unzip_dir, exist_ok=True)\n",
    "\n",
    "# Unzip the files\n",
    "for file in os.listdir(zip_dir):\n",
    "    if file.endswith('.zip'):\n",
    "        with zipfile.ZipFile(zip_dir + file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(unzip_dir)\n",
    "\n",
    "\n",
    "# Define the directory where the .wav and TRANSCRIPT.csv files will be stored\n",
    "final_dir = '../../data/raw/DAIC_WOZ_final/'\n",
    "\n",
    "# Delete the directory if it already exists\n",
    "shutil.rmtree(final_dir, ignore_errors=True)\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(final_dir, exist_ok=True)\n",
    "\n",
    "# Move the .wav and TRANSCRIPT.csv files to the final directory\n",
    "for root, dirs, files in os.walk(unzip_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.wav') or file.endswith('TRANSCRIPT.csv'):\n",
    "            shutil.move(os.path.join(root, file), final_dir)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice the audio data and match with transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download ffmpeg software and finish enviroment configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# Define the directory where the sliced .wav and .txt files are stored\n",
    "sliced_dir = '../../data/raw/DAIC_WOZ_sliced/'\n",
    "\n",
    "# Delete the directory if it already exists\n",
    "shutil.rmtree(sliced_dir, ignore_errors=True)\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(sliced_dir, exist_ok=True)\n",
    "\n",
    "# List the files in the directory\n",
    "os.listdir(final_dir)\n",
    "\n",
    "\n",
    "# If need, delete this file '._487_TRANSCRIPT.csv'\n",
    "try:\n",
    "    os.remove('../../data/raw/DAIC_WOZ_final/._487_TRANSCRIPT.csv')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Load the TRANSCRIPT.csv file, which contains the transcriptions of the audio files\n",
    "for file in os.listdir(final_dir):\n",
    "    if file.endswith('TRANSCRIPT.csv'):\n",
    "        # Define the audio file\n",
    "        audio_file = file.replace('TRANSCRIPT.csv', 'AUDIO.wav')\n",
    "        # Standardize audio files to a 16kHz sampling rate\n",
    "        os.system(f'ffmpeg -i {final_dir + audio_file} -ar 16000 {final_dir + \"16k\" + audio_file}')\n",
    "        # Load the TRANSCRIPT.csv file\n",
    "        transcript = pd.read_csv(final_dir + file, sep='\\t', header='infer')\n",
    "        # Set column names\n",
    "        transcript.columns = ['start_time', 'stop_time', 'role', 'text']\n",
    "        # Keep only the 'Participant' role\n",
    "        transcript = transcript[transcript['role'] == 'Participant']\n",
    "        # Keep only rows where the text is not empty\n",
    "        transcript = transcript[transcript['text'].notnull()]\n",
    "        # filter only start_time-stop_time >= 3\n",
    "        transcript = transcript[transcript['stop_time'] - transcript['start_time'] >= 3]\n",
    "        # Reset the index\n",
    "        transcript.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # For every row, save the text to a .txt file and slice the corresponding audio file\n",
    "        for i in range(len(transcript)):            \n",
    "            # Save the text to a .txt file\n",
    "            with open(sliced_dir + f'{file.split(\"_\")[0]}_{i}.txt', 'w') as f:\n",
    "                f.write(transcript['text'][i])\n",
    "            # Slice the audio file\n",
    "            start_time = transcript['start_time'][i]\n",
    "            stop_time = transcript['stop_time'][i]\n",
    "            # Convert the start and stop times to the correct format: HH:MM:SS\n",
    "            start_time = f'{int(start_time/3600):02d}:{int((start_time%3600)/60):02d}:{int(start_time%60):02d}'\n",
    "            stop_time = f'{int(stop_time/3600):02d}:{int((stop_time%3600)/60):02d}:{int(stop_time%60):02d}'\n",
    "            # Slice the audio file\n",
    "            os.system(f'ffmpeg -i {final_dir + \"16k\" + audio_file} -ss {start_time} -to {stop_time} {sliced_dir}{file.split(\"_\")[0]}_{i}.wav')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio and text data cleaning and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def text_data_cleaning(file_path):\n",
    "    # Read the text file, remove special characters and digits, convert the text to lowercase, and return the cleaned text to the text file\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "        # Remove special characters \n",
    "        text = re.sub(r'[^a-zA-Z\\s\\d]', '', text).strip()\n",
    "        # No need to remove stopwords\n",
    "        # Convert the text to lowercase\n",
    "        text = text.lower()\n",
    "    # Write the cleaned text to the text file\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(text)\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(sliced_dir):\n",
    "    if file.endswith('.txt'):\n",
    "        text_data_cleaning(sliced_dir + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q noisereduce\n",
    "# !pip install torchaudio\n",
    "# !pip install -q librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import os\n",
    "\n",
    "def clean_audio(input_path, output_path, target_sr=16000):\n",
    "    \"\"\"\n",
    "    数据清洗：\n",
    "    1. 采样率转换\n",
    "    2. 响度归一化\n",
    "    3. 降噪\n",
    "    4. 静音移除\n",
    "    \"\"\"\n",
    "    # 读取音频\n",
    "    audio, sr = librosa.load(input_path, sr=None)\n",
    "    \n",
    "    # 采样率转换\n",
    "    if sr != target_sr:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "    \n",
    "    # 响度归一化（标准化到 [-1, 1]）\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "\n",
    "    # 降噪处理\n",
    "    audio = nr.reduce_noise(y=audio, sr=target_sr)\n",
    "\n",
    "    # 静音移除，如果全部为静音，则删除该音频\n",
    "    non_silent_intervals = librosa.effects.split(audio, top_db=20)  # top_db 控制静音阈值\n",
    "    try:\n",
    "        audio = np.concatenate([audio[start:end] for start, end in non_silent_intervals])\n",
    "    except ValueError:\n",
    "        os.remove(input_path)\n",
    "        print(f'{input_path} is removed because it is silent.')\n",
    "        return\n",
    "\n",
    "    # 保存处理后的音频\n",
    "    sf.write(output_path, audio, target_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sora1874\\AppData\\Local\\Temp\\ipykernel_66192\\3695764594.py:27: RuntimeWarning: invalid value encountered in divide\n",
      "  audio = audio / np.max(np.abs(audio))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/raw/DAIC_WOZ_sliced/306_16.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/315_4.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/315_42.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/328_51.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/328_56.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/328_60.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/335_16.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/353_38.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/359_30.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/362_12.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/395_43.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/416_43.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/421_51.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/429_20.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/434_45.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/465_80.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/470_27.wav is removed because it is silent.\n",
      "../../data/raw/DAIC_WOZ_sliced/481_14.wav is removed because it is silent.\n"
     ]
    }
   ],
   "source": [
    "sliced_dir = '../../data/raw/DAIC_WOZ_sliced/'\n",
    "cleaned_dir = '../../data/raw/DAIC_WOZ_cleaned/'\n",
    "# Delete the directory if it already exists\n",
    "shutil.rmtree(cleaned_dir)\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(cleaned_dir, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(sliced_dir):\n",
    "    if file.endswith('.wav'):\n",
    "        clean_audio(sliced_dir + file, cleaned_dir + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(input_path, output_path, target_sr=16000):\n",
    "    \"\"\"\n",
    "    数据增强：\n",
    "    1. 时间拉伸 (Time Stretching)\n",
    "    2. 音高变化 (Pitch Shifting)\n",
    "    3. 噪声注入 (Noise Augmentation)\n",
    "    4. 频谱增强 (SpecAugment)\n",
    "    \"\"\"\n",
    "    # 读取音频\n",
    "    audio, sr = librosa.load(input_path, sr=target_sr)\n",
    "\n",
    "    # 1. 时间拉伸\n",
    "    if random.random() > 0.5:\n",
    "        rate = random.uniform(0.8, 1.2)  # 在 0.8x ~ 1.2x 之间变化\n",
    "        audio = librosa.effects.time_stretch(audio, rate=rate)\n",
    "    \n",
    "    # 2. 音高变化\n",
    "    if random.random() > 0.5:\n",
    "        steps = random.randint(-2, 2)  # 随机上下变 2 个半音\n",
    "        audio = librosa.effects.pitch_shift(audio, sr=target_sr, n_steps=steps)\n",
    "\n",
    "    # 3. 噪声注入\n",
    "    if random.random() > 0.5:\n",
    "        noise = np.random.normal(0, 0.005, audio.shape)  # 添加高斯噪声\n",
    "        audio = audio + noise\n",
    "        audio = np.clip(audio, -1.0, 1.0)  # 防止超出 [-1, 1]\n",
    "\n",
    "    # 4. SpecAugment（使用 torchaudio 实现）\n",
    "    if random.random() > 0.5:\n",
    "        audio_tensor = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)  # 转换为 PyTorch Tensor\n",
    "        mel_spec = T.MelSpectrogram(sample_rate=target_sr)(audio_tensor)\n",
    "        spec_aug = T.FrequencyMasking(freq_mask_param=30)(mel_spec)  # 频率屏蔽\n",
    "        spec_aug = T.TimeMasking(time_mask_param=50)(spec_aug)  # 时间屏蔽\n",
    "        audio = librosa.istft(librosa.db_to_amplitude(spec_aug.squeeze(0).numpy()))  # 逆变换回时域信号\n",
    "\n",
    "    # 保存增强后的音频\n",
    "    sf.write(output_path, audio, target_sr)\n",
    "\n",
    "# 示例调用\n",
    "# clean_audio(\"input.wav\", \"cleaned.wav\")\n",
    "# augment_audio(\"cleaned.wav\", \"augmented.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sora1874\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sora1874\\AppData\\Roaming\\Python\\Python39\\site-packages\\librosa\\core\\spectrum.py:1873: RuntimeWarning: overflow encountered in power\n",
      "  return ref * np.power(10.0, S_db * 0.1)\n",
      "C:\\Users\\Sora1874\\AppData\\Roaming\\Python\\Python39\\site-packages\\librosa\\core\\spectrum.py:598: RuntimeWarning: invalid value encountered in multiply\n",
      "  ytmp = ifft_window * fft.irfft(stft_matrix[..., bl_s:bl_t], n=n_fft, axis=-2)\n",
      "C:\\Users\\Sora1874\\AppData\\Roaming\\Python\\Python39\\site-packages\\librosa\\core\\spectrum.py:566: RuntimeWarning: invalid value encountered in multiply\n",
      "  ytmp = ifft_window * fft.irfft(stft_matrix[..., :start_frame], n=n_fft, axis=-2)\n",
      "C:\\Users\\Sora1874\\AppData\\Roaming\\Python\\Python39\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1536\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "DAIC_WOZ_augmented_dir = '../../data/raw/DAIC_WOZ_augmented/'\n",
    "\n",
    "# Delete the directory if it already exists\n",
    "shutil.rmtree(DAIC_WOZ_augmented_dir)\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(DAIC_WOZ_augmented_dir, exist_ok=True)\n",
    "\n",
    "# Set the random seed\n",
    "random.seed(42)\n",
    "\n",
    "# Augment the audio files\n",
    "for file in os.listdir('../../data/raw/DAIC_WOZ_cleaned/'):\n",
    "    augment_audio(f'../../data/raw/DAIC_WOZ_cleaned/{file}', f'{DAIC_WOZ_augmented_dir}{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the augmented audio files and their corresponding text files to the processed directory\n",
    "processed_audio_dir = '../../data/processed/DAIC_WOZ/audio/'\n",
    "processed_text_dir = '../../data/processed/DAIC_WOZ/text/'\n",
    "# Delete the directory if it already exists\n",
    "shutil.rmtree(processed_audio_dir, ignore_errors=True)\n",
    "shutil.rmtree(processed_text_dir, ignore_errors=True)\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(processed_audio_dir, exist_ok=True)\n",
    "os.makedirs(processed_text_dir, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(DAIC_WOZ_augmented_dir):\n",
    "    if file.endswith('.wav'):\n",
    "        shutil.move(DAIC_WOZ_augmented_dir + file, processed_audio_dir + file)\n",
    "        # Move the corresponding text file\n",
    "        try:\n",
    "            shutil.move(sliced_dir + file.replace('.wav', '.txt'), processed_text_dir + file.replace('.wav', '.txt'))\n",
    "        except FileNotFoundError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets at a ratio of 80:10:10, and copy the files to the corresponding splits directorie\n",
    "# e.g. data/splits/DAIC_WOZ/{split}/audio\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "processed_audio_dir = '../../data/processed/DAIC_WOZ/audio/'\n",
    "processed_text_dir = '../../data/processed/DAIC_WOZ/text/'\n",
    "\n",
    "\n",
    "data_split_list = ['train', 'val', 'test']\n",
    "for split in data_split_list:\n",
    "    # Delete the directory if it already exists\n",
    "    shutil.rmtree(f'../../data/splits/DAIC_WOZ/{split}/audio/', ignore_errors=True)\n",
    "    shutil.rmtree(f'../../data/splits/DAIC_WOZ/{split}/text/', ignore_errors=True)\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(f'../../data/splits/DAIC_WOZ/{split}/audio/', exist_ok=True)\n",
    "    os.makedirs(f'../../data/splits/DAIC_WOZ/{split}/text/', exist_ok=True)\n",
    "\n",
    "\n",
    "# Set the random seed\n",
    "random.seed(42)\n",
    "\n",
    "# Randomly assign the files to the training, validation, and test sets, and copy the files to the corresponding directories\n",
    "for file in os.listdir(processed_audio_dir):\n",
    "    split = random.choices(data_split_list, weights=[0.8, 0.1, 0.1], k=1)[0]\n",
    "    shutil.copy(processed_audio_dir + file, f'../../data/splits/DAIC_WOZ/{split}/audio/{file}')\n",
    "    try:\n",
    "        shutil.copy(processed_text_dir + file.replace('.wav', '.txt'), f'../../data/splits/DAIC_WOZ/{split}/text/{file.replace(\".wav\", \".txt\")}')\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the directories that are no longer needed\n",
    "try:\n",
    "    shutil.rmtree('../../data/raw/DAIC_WOZ_unzipped/')\n",
    "    shutil.rmtree('../../data/raw/DAIC_WOZ_final/')\n",
    "    shutil.rmtree('../../data/raw/DAIC_WOZ_sliced/')\n",
    "    shutil.rmtree('../../data/raw/DAIC_WOZ_cleaned/')\n",
    "    shutil.rmtree('../../data/raw/DAIC_WOZ_augmented/')\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAVDESS preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: unmatched '(' (2858528579.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[50], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    with open(f'.../data/raw/{RAVDESS_url.split('/')[-1].split('?')[0]}', 'wb') as f:\u001b[0m\n\u001b[1;37m                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m f-string: unmatched '('\n"
     ]
    }
   ],
   "source": [
    "# Download the RAVDESS dataset\n",
    "RAVDESS_url = 'https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip?download=1'\n",
    "r = requests.get(RAVDESS_url, stream=True)\n",
    "with open(f'.../data/raw/Audio_Speech_Actors_01-24.zip', 'wb') as f:\n",
    "    for chunk in r.iter_content(chunk_size=8192):\n",
    "        if chunk:\n",
    "            f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the downloaded zip files and only keep the .wav and TRANSCRIPT.csv files\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the directory where the zip files are stored\n",
    "zip_dir = '../../data/raw/'\n",
    "\n",
    "# Define the directory where the unzipped files will be stored\n",
    "unzip_dir = '../../data/raw/Audio_Speech_Actors_01-24/'\n",
    "\n",
    "# Delete the directory if it already exists\n",
    "shutil.rmtree(unzip_dir, ignore_errors=True)\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(unzip_dir, exist_ok=True)\n",
    "\n",
    "# Unzip the files\n",
    "file = 'Audio_Speech_Actors_01-24.zip'\n",
    "with zipfile.ZipFile(zip_dir + file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(unzip_dir)\n",
    "# Delete the zip file\n",
    "# os.remove(zip_dir + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the raw files are stored\n",
    "RAVDESS_raw_dir = '../../data/raw/RAVDESS/'\n",
    "\n",
    "# Delete the directory if it already exists\n",
    "shutil.rmtree(RAVDESS_raw_dir, ignore_errors=True)\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(RAVDESS_raw_dir, exist_ok=True)\n",
    "\n",
    "# For all the folders in the unzipped directory, move the .wav files to the RAVDESS_raw_dir\n",
    "for folder in os.listdir(unzip_dir):\n",
    "    for file in os.listdir(unzip_dir + folder):\n",
    "        if file.endswith('.wav'):\n",
    "            shutil.move(unzip_dir + folder + '/' + file, RAVDESS_raw_dir)\n",
    "\n",
    "# Delete the unzipped directory\n",
    "# shutil.rmtree(unzip_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "Vocal channel (01 = speech, 02 = song).\n",
    "Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "Actor (01 to 24. Odd numbered actors are male, even numbered actors are female)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the labeled files are stored\n",
    "RAVDESS_labeled_dir = '../../data/raw/RAVDESS_labeled/'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(RAVDESS_labeled_dir, exist_ok=True)\n",
    "\n",
    "# Label the RAVDESS dataset\n",
    "# Define the emotions: Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised)\n",
    "emotions = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
    "\n",
    "# For all the files in the RAVDESS_raw_dir, modify the name to be original name+label, and move the files to the RAVDESS_labeled_dir with the correct emotion label\n",
    "for file in os.listdir(RAVDESS_raw_dir):\n",
    "    # Get the emotion label\n",
    "    emotion = emotions[int(file.split('-')[2]) - 1]\n",
    "    # Modify the name\n",
    "    new_name = file.split('.')[0] + '-' + emotion + '.wav'\n",
    "    # Move the file\n",
    "    shutil.move(RAVDESS_raw_dir + file, RAVDESS_labeled_dir + new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sora1874\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sora1874\\AppData\\Roaming\\Python\\Python39\\site-packages\\librosa\\core\\spectrum.py:1873: RuntimeWarning: overflow encountered in power\n",
      "  return ref * np.power(10.0, S_db * 0.1)\n",
      "C:\\Users\\Sora1874\\AppData\\Roaming\\Python\\Python39\\site-packages\\librosa\\core\\spectrum.py:598: RuntimeWarning: invalid value encountered in multiply\n",
      "  ytmp = ifft_window * fft.irfft(stft_matrix[..., bl_s:bl_t], n=n_fft, axis=-2)\n"
     ]
    }
   ],
   "source": [
    "# Audio cleaning and augmentation\n",
    "\n",
    "RAVDESS_labeled_dir = '../../data/raw/RAVDESS_labeled/'\n",
    "\n",
    "# Define the directory where the cleaned audio files will be stored\n",
    "RAVDESS_cleaned_dir = '../../data/raw/RAVDESS_cleaned/'\n",
    "\n",
    "\n",
    "# Delete the directory if it already exists\n",
    "shutil.rmtree(RAVDESS_cleaned_dir, ignore_errors=True)\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(RAVDESS_cleaned_dir, exist_ok=True)\n",
    "\n",
    "# Clean the audio files\n",
    "for file in os.listdir(RAVDESS_labeled_dir):\n",
    "    clean_audio(RAVDESS_labeled_dir + file, RAVDESS_cleaned_dir + file)\n",
    "\n",
    "# Define the directory where the augmented audio files will be stored\n",
    "RAVDESS_augmented_dir = '../../data/raw/RAVDESS_augmented/'\n",
    "\n",
    "# Delete the directory if it already exists\n",
    "shutil.rmtree(RAVDESS_augmented_dir, ignore_errors=True)\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(RAVDESS_augmented_dir, exist_ok=True)\n",
    "\n",
    "# Augment the audio files\n",
    "for file in os.listdir(RAVDESS_cleaned_dir):\n",
    "    augment_audio(RAVDESS_cleaned_dir + file, RAVDESS_augmented_dir + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the augmented audio files to the processed directory\n",
    "processed_audio_dir = '../../data/processed/RAVDESS/audio/'\n",
    "\n",
    "# Delete the directory if it already exists\n",
    "shutil.rmtree(processed_audio_dir, ignore_errors=True)\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(processed_audio_dir, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(RAVDESS_augmented_dir):\n",
    "    shutil.move(RAVDESS_augmented_dir + file, processed_audio_dir + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets at a ratio of 80:10:10, and copy the files to the corresponding splits directorie\n",
    "# e.g. data/splits/RAVDESS/{split}/audio\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "processed_audio_dir = '../../data/processed/RAVDESS/audio/'\n",
    "\n",
    "data_split_list = ['train', 'val', 'test']\n",
    "for split in data_split_list:\n",
    "    # Delete the directory if it already exists\n",
    "    shutil.rmtree(f'../../data/splits/RAVDESS/{split}/audio/', ignore_errors=True)\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(f'../../data/splits/RAVDESS/{split}/audio/', exist_ok=True)\n",
    "\n",
    "# Set the random seed\n",
    "random.seed(42)\n",
    "\n",
    "# Randomly assign the files to the training, validation, and test sets, and copy the files to the corresponding directories\n",
    "for file in os.listdir(processed_audio_dir):\n",
    "    split = random.choices(data_split_list, weights=[0.8, 0.1, 0.1], k=1)[0]\n",
    "    shutil.copy(processed_audio_dir + file, f'../../data/splits/RAVDESS/{split}/audio/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the directories that are no longer needed\n",
    "try:\n",
    "    shutil.rmtree('../../data/raw/RAVDESS/')\n",
    "    shutil.rmtree('../../data/raw/RAVDESS_cleaned/')\n",
    "    shutil.rmtree('../../data/raw/RAVDESS_augmented/')\n",
    "    shutil.rmtree('../../data/raw/RAVDESS_labeled/')\n",
    "    shutil.rmtree('../../data/raw/Audio_Speech_Actors_01-24/')\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MELD preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the MELD dataset\n",
    "MELD_url = 'https://huggingface.co/datasets/declare-lab/MELD/resolve/main/MELD.Raw.tar.gz'\n",
    "r = requests.get(MELD_url, stream=True)\n",
    "with open(f'../../data/raw/MELD.Raw.tar.gz', 'wb') as f:\n",
    "    for chunk in r.iter_content(chunk_size=8192):\n",
    "        if chunk:\n",
    "            f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# 解压缩 .tar.gz 文件\n",
    "def extract_tar_gz(file_path, extract_path):\n",
    "    with tarfile.open(file_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extract_path)\n",
    "\n",
    "# 示例\n",
    "file_path = '../../data/raw/MELD.Raw.tar.gz'\n",
    "extract_path = '../../data/raw/MELD_raw/'\n",
    "\n",
    "# Delete the directory if it already exists\n",
    "shutil.rmtree(extract_path, ignore_errors=True)\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "extract_tar_gz(file_path, extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countinue to extract the .tar.gz files\n",
    "split_list = ['train', 'dev', 'test']\n",
    "for split in split_list:\n",
    "    file_path = f'../../data/raw/MELD_raw/MELD.Raw/{split}.tar.gz'\n",
    "    # Delete the directory if it already exists\n",
    "    shutil.rmtree(f'../../data/raw/MELD_raw/MELD.Raw/{split}/', ignore_errors=True)\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(f'../../data/raw/MELD_raw/MELD.Raw/{split}/', exist_ok=True)\n",
    "    extract_path = f'../../data/raw/MELD_raw/MELD.Raw/{split}/'\n",
    "    extract_tar_gz(file_path, extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the dev_sent_emo.csv and test_sent_emo.csv to corresponding directories\n",
    "try:\n",
    "    shutil.move(f'../../data/raw/MELD_raw/MELD.Raw/dev_sent_emo.csv', f'../../data/raw/MELD_raw/MELD.Raw/dev/dev_sent_emo.csv')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "try:\n",
    "    shutil.move(f'../../data/raw/MELD_raw/MELD.Raw/test_sent_emo.csv', f'../../data/raw/MELD_raw/MELD.Raw/test/test_sent_emo.csv')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "# Change the folder name to the correct name, e.g. dev_splits_complete -> dev_splits, output_repeated_splits_test -> test_splits\n",
    "try:\n",
    "    os.rename('../../data/raw/MELD_raw/MELD.Raw/dev/dev_splits_complete', '../../data/raw/MELD_raw/MELD.Raw/dev/dev_splits')\n",
    "    os.rename('../../data/raw/MELD_raw/MELD.Raw/test/output_repeated_splits_test', '../../data/raw/MELD_raw/MELD.Raw/test/test_splits')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "# Delete files not in the correct format, e.g. not starting with 'dia'\n",
    "\n",
    "try:\n",
    "    for root, dirs, files in os.walk('../../data/raw/MELD_raw/MELD.Raw/test/test_splits/'):\n",
    "        for file in files:\n",
    "            if file.startswith('dia') == False:\n",
    "                os.remove(os.path.join(root, file))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the splitted files with information from the .csv files, and move the labeled audio files to the labeled directory\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "split_list = ['train', 'dev', 'test']\n",
    "\n",
    "# Convert the formats of the start and stop times format: from HH:MM:SS to seconds\n",
    "def convert_time_MELD(time_1, time_2):\n",
    "    h, m, s = time_1.split(':')\n",
    "    ms = time_2\n",
    "    return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in split_list:\n",
    "    # Define the directory where the raw files are stored\n",
    "    MELD_raw_dir = f'../../data/raw/MELD_raw/MELD.Raw/{split}/{split}_splits/'\n",
    "\n",
    "    # Define the directory where the labeled files are stored\n",
    "    MELD_labeled_audio_dir = f'../../data/raw/MELD_labeled/{split}/audio/'\n",
    "    MELD_labeled_text_dir = f'../../data/raw/MELD_labeled/{split}/text/'\n",
    "\n",
    "    # Delete the directory if it already exists\n",
    "    shutil.rmtree(MELD_labeled_audio_dir, ignore_errors=True)\n",
    "    shutil.rmtree(MELD_labeled_text_dir, ignore_errors=True)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(MELD_labeled_audio_dir, exist_ok=True)\n",
    "    os.makedirs(MELD_labeled_text_dir, exist_ok=True)\n",
    "\n",
    "    # Load the .csv file\n",
    "    df = pd.read_csv(f'../../data/raw/MELD_raw/MELD.Raw/{split}/{split}_sent_emo.csv', header='infer')\n",
    "\n",
    "    # For all the files in the MELD_raw_dir, modify the name to be original name+label, generate the transcripts txt file, and move the files to the MELD_labeled_dir with the correct emotion label\n",
    "    for file in os.listdir(MELD_raw_dir):\n",
    "        dia_num = file.split('_')[0].split('dia')[1]\n",
    "        utt_num = file.split('_')[1].split('utt')[1].split('.')[0]\n",
    "        # Filter if duration > 3s， if can't find the start_time, skip\n",
    "        if len(df[(df['Dialogue_ID'] == int(dia_num)) & (df['Utterance_ID'] == int(utt_num))]) > 0:\n",
    "            start_time = df[(df['Dialogue_ID'] == int(dia_num)) & (df['Utterance_ID'] == int(utt_num))]['StartTime'].values[0]\n",
    "            end_time = df[(df['Dialogue_ID'] == int(dia_num)) & (df['Utterance_ID'] == int(utt_num))]['EndTime'].values[0]\n",
    "            start_time_in_sec = convert_time_MELD( start_time.split(',')[0], start_time.split(',')[1])\n",
    "            end_time_in_sec = convert_time_MELD( end_time.split(',')[0], end_time.split(',')[1])\n",
    "            if end_time_in_sec - start_time_in_sec > 3:\n",
    "                # Get the emotion label\n",
    "                emotion = df[(df['Dialogue_ID'] == int(dia_num)) & (df['Utterance_ID'] == int(utt_num))]['Emotion'].values[0]\n",
    "                # Modify the name\n",
    "                audio_new_name = file.split('.')[0] + '_' + emotion + '.wav'\n",
    "                text_new_name = file.split('.')[0] + '_' + emotion + '.txt'\n",
    "                # retrieve the transcript\n",
    "                transcript = df[(df['Dialogue_ID'] == int(dia_num)) & (df['Utterance_ID'] == int(utt_num))]['Utterance'].values[0]\n",
    "                # Transfer the .mp4 file to .wav file and move the file\n",
    "                os.system(f'ffmpeg -i {MELD_raw_dir + file} -ar 16000 {MELD_labeled_audio_dir + audio_new_name}')\n",
    "                # Write the transcript to a text file\n",
    "                with open(MELD_labeled_text_dir + text_new_name, 'w') as f:\n",
    "                    f.write(transcript)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sora1874\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Sora1874\\AppData\\Roaming\\Python\\Python39\\site-packages\\librosa\\core\\spectrum.py:1873: RuntimeWarning: overflow encountered in power\n",
      "  return ref * np.power(10.0, S_db * 0.1)\n",
      "C:\\Users\\Sora1874\\AppData\\Roaming\\Python\\Python39\\site-packages\\librosa\\core\\spectrum.py:598: RuntimeWarning: invalid value encountered in multiply\n",
      "  ytmp = ifft_window * fft.irfft(stft_matrix[..., bl_s:bl_t], n=n_fft, axis=-2)\n",
      "C:\\Users\\Sora1874\\AppData\\Roaming\\Python\\Python39\\site-packages\\librosa\\core\\spectrum.py:566: RuntimeWarning: invalid value encountered in multiply\n",
      "  ytmp = ifft_window * fft.irfft(stft_matrix[..., :start_frame], n=n_fft, axis=-2)\n"
     ]
    }
   ],
   "source": [
    "# Audio cleaning and augmentation\n",
    "for split in split_list:\n",
    "    # Define the directory where the cleaned audio files will be stored\n",
    "    MELD_cleaned_dir = f'../../data/raw/MELD_cleaned/{split}/'\n",
    "\n",
    "    # Delete the directory if it already exists\n",
    "    shutil.rmtree(MELD_cleaned_dir, ignore_errors=True)\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(MELD_cleaned_dir, exist_ok=True)\n",
    "\n",
    "    # Clean the audio files\n",
    "    for file in os.listdir(f'../../data/raw/MELD_labeled/{split}/audio/'):\n",
    "        clean_audio(f'../../data/raw/MELD_labeled/{split}/audio/{file}', f'{MELD_cleaned_dir}{file}')\n",
    "\n",
    "    # Define the directory where the augmented audio files will be stored\n",
    "    MELD_augmented_dir = f'../../data/raw/MELD_augmented/{split}/'\n",
    "\n",
    "    # Delete the directory if it already exists\n",
    "    shutil.rmtree(MELD_augmented_dir, ignore_errors=True)\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(MELD_augmented_dir, exist_ok=True)\n",
    "\n",
    "    # Augment the audio files\n",
    "    for file in os.listdir(MELD_cleaned_dir):\n",
    "        augment_audio(MELD_cleaned_dir + file, MELD_augmented_dir + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text data cleaning\n",
    "for split in split_list:\n",
    "    for file in os.listdir(f'../../data/raw/MELD_labeled/{split}/text/'):\n",
    "        text_data_cleaning(f'../../data/raw/MELD_labeled/{split}/text/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the audio and text files to the processed data directory\n",
    "for split in split_list:\n",
    "    processed_audio_dir = f'../../data/processed/MELD/{split}/audio/'\n",
    "    processed_text_dir = f'../../data/processed/MELD/{split}/text/'\n",
    "    # Delete the directory if it already exists\n",
    "    shutil.rmtree(processed_audio_dir, ignore_errors=True)\n",
    "    shutil.rmtree(processed_text_dir, ignore_errors=True)\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(processed_audio_dir, exist_ok=True)\n",
    "    os.makedirs(processed_text_dir, exist_ok=True)\n",
    "    for file in os.listdir(f'../../data/raw/MELD_augmented/{split}/'):\n",
    "        if file.endswith('.wav'):\n",
    "            shutil.move(f'../../data/raw/MELD_augmented/{split}/{file}', processed_audio_dir + file)\n",
    "            # Move the corresponding text file\n",
    "            try:\n",
    "                shutil.move(f'../../data/raw/MELD_labeled/{split}/text/{file.replace(\".wav\", \".txt\")}', processed_text_dir + file.replace('.wav', '.txt'))\n",
    "            except FileNotFoundError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the dev directory to val\n",
    "os.rename('../../data/processed/MELD/dev', '../../data/processed/MELD/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/splits/MELD'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy the splitted MELD dataset to splits directory\n",
    "shutil.copytree('../../data/processed/MELD', '../../data/splits/MELD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the directories that are no longer needed\n",
    "try:\n",
    "    shutil.rmtree('../../data/raw/MELD_raw/')\n",
    "    shutil.rmtree('../../data/raw/MELD_labeled/')\n",
    "    shutil.rmtree('../../data/raw/MELD_cleaned/')\n",
    "    shutil.rmtree('../../data/raw/MELD_augmented/')\n",
    "except FileNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate transcripts for Some Audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai-whisper\n",
      "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
      "     ---------------------------------------- 0.0/800.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/800.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/800.5 kB ? eta -:--:--\n",
      "     ------------- -------------------------- 262.1/800.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 800.5/800.5 kB 2.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numba in c:\\users\\sora1874\\appdata\\roaming\\python\\python39\\site-packages (from openai-whisper) (0.60.0)\n",
      "Requirement already satisfied: numpy in c:\\personalfile\\d\\coding\\anaconda\\anaconda\\envs\\d2l\\lib\\site-packages (from openai-whisper) (1.23.5)\n",
      "Requirement already satisfied: torch in c:\\users\\sora1874\\appdata\\roaming\\python\\python39\\site-packages (from openai-whisper) (2.6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sora1874\\appdata\\roaming\\python\\python39\\site-packages (from openai-whisper) (4.67.1)\n",
      "Collecting more-itertools (from openai-whisper)\n",
      "  Downloading more_itertools-10.6.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting tiktoken (from openai-whisper)\n",
      "  Downloading tiktoken-0.9.0-cp39-cp39-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\sora1874\\appdata\\roaming\\python\\python39\\site-packages (from numba->openai-whisper) (0.43.0)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->openai-whisper)\n",
      "  Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\personalfile\\d\\coding\\anaconda\\anaconda\\envs\\d2l\\lib\\site-packages (from tiktoken->openai-whisper) (2.31.0)\n",
      "Requirement already satisfied: filelock in c:\\personalfile\\d\\coding\\anaconda\\anaconda\\envs\\d2l\\lib\\site-packages (from torch->openai-whisper) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\personalfile\\d\\coding\\anaconda\\anaconda\\envs\\d2l\\lib\\site-packages (from torch->openai-whisper) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\personalfile\\d\\coding\\anaconda\\anaconda\\envs\\d2l\\lib\\site-packages (from torch->openai-whisper) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\personalfile\\d\\coding\\anaconda\\anaconda\\envs\\d2l\\lib\\site-packages (from torch->openai-whisper) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sora1874\\appdata\\roaming\\python\\python39\\site-packages (from torch->openai-whisper) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sora1874\\appdata\\roaming\\python\\python39\\site-packages (from torch->openai-whisper) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\personalfile\\d\\coding\\anaconda\\anaconda\\envs\\d2l\\lib\\site-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\personalfile\\d\\coding\\anaconda\\anaconda\\envs\\d2l\\lib\\site-packages (from tqdm->openai-whisper) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\personalfile\\d\\coding\\anaconda\\anaconda\\envs\\d2l\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\personalfile\\d\\coding\\anaconda\\anaconda\\envs\\d2l\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\personalfile\\d\\coding\\anaconda\\anaconda\\envs\\d2l\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\personalfile\\d\\coding\\anaconda\\anaconda\\envs\\d2l\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\personalfile\\d\\coding\\anaconda\\anaconda\\envs\\d2l\\lib\\site-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
      "Downloading more_itertools-10.6.0-py3-none-any.whl (63 kB)\n",
      "Downloading tiktoken-0.9.0-cp39-cp39-win_amd64.whl (894 kB)\n",
      "   ---------------------------------------- 0.0/894.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 894.2/894.2 kB 4.1 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml): started\n",
      "  Building wheel for openai-whisper (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803407 sha256=860b27eae5768e0931c59907e4bbfee361b4547f21cae24a6edcd3efa66c9816\n",
      "  Stored in directory: c:\\users\\sora1874\\appdata\\local\\pip\\cache\\wheels\\94\\29\\f3\\3dd4d7f88df5d701acd3206732dcb6265379c5ece94b472c17\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: regex, more-itertools, tiktoken, openai-whisper\n",
      "Successfully installed more-itertools-10.6.0 openai-whisper-20240930 regex-2024.11.6 tiktoken-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script whisper.exe is installed in 'C:\\Users\\Sora1874\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2.88G/2.88G [02:23<00:00, 21.5MiB/s]\n",
      "C:\\Users\\Sora1874\\AppData\\Roaming\\Python\\Python39\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     28\u001b[0m     audio_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(RAVDESS_labeled_dir, filename)\n\u001b[1;32m---> 29\u001b[0m     transcript \u001b[38;5;241m=\u001b[39m \u001b[43mtranscribe_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# 保存到 .txt 文件（与音频文件同名）\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     txt_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(filename)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[55], line 22\u001b[0m, in \u001b[0;36mtranscribe_audio\u001b[1;34m(audio_path)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtranscribe_audio\u001b[39m(audio_path):\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" 使用 Whisper-large-v3 转录音频 \"\"\"\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\whisper\\transcribe.py:146\u001b[0m, in \u001b[0;36mtranscribe\u001b[1;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetecting language using up to the first 30 seconds. Use `--language` to specify the language\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m     )\n\u001b[0;32m    145\u001b[0m mel_segment \u001b[38;5;241m=\u001b[39m pad_or_trim(mel, N_FRAMES)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(dtype)\n\u001b[1;32m--> 146\u001b[0m _, probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_language\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(probs, key\u001b[38;5;241m=\u001b[39mprobs\u001b[38;5;241m.\u001b[39mget)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\whisper\\decoding.py:52\u001b[0m, in \u001b[0;36mdetect_language\u001b[1;34m(model, mel, tokenizer)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# skip encoder forward pass if already-encoded audio features were given\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m!=\u001b[39m (model\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_ctx, model\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_state):\n\u001b[1;32m---> 52\u001b[0m     mel \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# forward pass using a single token, startoftranscript\u001b[39;00m\n\u001b[0;32m     55\u001b[0m n_audio \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\whisper\\model.py:201\u001b[0m, in \u001b[0;36mAudioEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    198\u001b[0m x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m--> 201\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_post(x)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\whisper\\model.py:167\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[1;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    162\u001b[0m     x: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m     kv_cache: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    166\u001b[0m ):\n\u001b[1;32m--> 167\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_ln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn:\n\u001b[0;32m    169\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn_ln(x), xa, kv_cache\u001b[38;5;241m=\u001b[39mkv_cache)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\whisper\\model.py:99\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x, xa, mask, kv_cache)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     94\u001b[0m     x: Tensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m     kv_cache: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     98\u001b[0m ):\n\u001b[1;32m---> 99\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m xa \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kv_cache:\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;66;03m# hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\u001b[39;00m\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;66;03m# otherwise, perform key/value projections for self- or cross-attention as usual.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(x \u001b[38;5;28;01mif\u001b[39;00m xa \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m xa)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\whisper\\model.py:46\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate transcripts for the RAVDESS dataset\n",
    "# Still use the RAVDESS_labeled_dir as the directory where the transcripts are stored\n",
    "# For all the files in the RAVDESS_labeled_dir, generate a transcript using ASR model such as Whisper-large-v3, and save it to a .txt file\n",
    "import os\n",
    "import torch\n",
    "import whisper\n",
    "\n",
    "# 设置输入和输出目录\n",
    "RAVDESS_labeled_dir = '../../data/raw/RAVDESS_labeled/'\n",
    "output_dir = '../../data/raw/RAVDESS_transcripts/'\n",
    "\n",
    "# Delete the directory if it already exists\n",
    "shutil.rmtree(output_dir, ignore_errors=True)\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 加载 Whisper-large-v3 模型\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = whisper.load_model(\"large-v3\").to(device)\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\" 使用 Whisper-large-v3 转录音频 \"\"\"\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "# 遍历 RAVDESS 目录中的所有 WAV 文件\n",
    "for filename in os.listdir(RAVDESS_labeled_dir):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        audio_path = os.path.join(RAVDESS_labeled_dir, filename)\n",
    "        transcript = transcribe_audio(audio_path)\n",
    "\n",
    "        # 保存到 .txt 文件（与音频文件同名）\n",
    "        txt_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "        txt_path = os.path.join(output_dir, txt_filename)\n",
    "\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(transcript)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
