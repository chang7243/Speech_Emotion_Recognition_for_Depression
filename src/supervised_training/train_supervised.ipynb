{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Supervised Fine-Tuning on RAVDESS (Speech-Only)\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Dataset Class for RAVDESS\n",
    "def augment_audio(audio, sr):\n",
    "    # SpecAugment: Time Masking + Frequency Masking\n",
    "    audio_tensor = torch.tensor(audio)\n",
    "    time_masking = torchaudio.transforms.TimeMasking(time_mask_param=80)\n",
    "    freq_masking = torchaudio.transforms.FrequencyMasking(freq_mask_param=30)\n",
    "    augmented_audio = time_masking(audio_tensor.unsqueeze(0))\n",
    "    augmented_audio = freq_masking(augmented_audio).squeeze(0)\n",
    "    return augmented_audio.numpy()\n",
    "\n",
    "class RAVDESSDataset(Dataset):\n",
    "    def __init__(self, data_dir, processor):\n",
    "        self.files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.wav')]\n",
    "        self.labels = [int(f.split('-')[2]) - 1 for f in os.listdir(data_dir) if f.endswith('.wav')]\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio, sr = librosa.load(self.files[idx], sr=16000)\n",
    "        audio = augment_audio(audio, sr)\n",
    "        inputs = self.processor(audio, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "        return inputs.squeeze(0), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Training Function\n",
    "def train_speech_model():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base\", num_labels=8).to(device)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "    dataset = RAVDESSDataset(\"data/processed/RAVDESS\", processor)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(10):\n",
    "        total_loss = 0\n",
    "        for batch, labels in dataloader:\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "            outputs = model(batch).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss/len(dataloader)}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), \"models/fine_tuned_speech_model.pth\")\n",
    "    print(\"Supervised Fine-Tuning Completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_speech_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-modal part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Multimodal Emotion Recognition (Speech + Text)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import Wav2Vec2Model, BertModel, Wav2Vec2Processor, BertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Dataset Class for Multimodal Training\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, audio_dir, text_path, processor, tokenizer):\n",
    "        self.audio_files = [os.path.join(audio_dir, f) for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
    "        self.transcripts = pd.read_csv(text_path)\n",
    "        self.labels = self.transcripts['label'].values\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio, sr = librosa.load(self.audio_files[idx], sr=16000)\n",
    "        audio_inputs = self.processor(audio, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
    "        text_inputs = self.tokenizer(self.transcripts.iloc[idx]['text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "        return audio_inputs.squeeze(0), text_inputs['input_ids'].squeeze(0), text_inputs['attention_mask'].squeeze(0), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Multimodal Model (Wav2Vec2 + BERT + Cross-Modal Attention)\n",
    "class MultimodalEmotionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultimodalEmotionModel, self).__init__()\n",
    "        self.audio_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        self.text_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=768, num_heads=8)\n",
    "        self.fc = nn.Linear(768, 8)\n",
    "    \n",
    "    def forward(self, audio_input, text_input, text_mask):\n",
    "        audio_features = self.audio_model(audio_input).last_hidden_state[:, 0, :]\n",
    "        text_features = self.text_model(text_input, attention_mask=text_mask).last_hidden_state[:, 0, :]\n",
    "        combined_features, _ = self.cross_attention(audio_features.unsqueeze(1), text_features.unsqueeze(1), text_features.unsqueeze(1))\n",
    "        logits = self.fc(combined_features.squeeze(1))\n",
    "        return logits\n",
    "\n",
    "# Training Function\n",
    "def train_multimodal_model():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MultimodalEmotionModel().to(device)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    dataset = MultimodalDataset(\"data/processed/audio\", \"data/processed/text.csv\", processor, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(10):\n",
    "        total_loss = 0\n",
    "        for audio_batch, text_batch, text_mask, labels in dataloader:\n",
    "            audio_batch, text_batch, text_mask, labels = audio_batch.to(device), text_batch.to(device), text_mask.to(device), labels.to(device)\n",
    "            outputs = model(audio_batch, text_batch, text_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss/len(dataloader)}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), \"models/multimodal_emotion_model.pth\")\n",
    "    print(\"Multimodal Training Completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_multimodal_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
